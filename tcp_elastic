#include <linux/module.h>
#include <net/tcp.h>

#include <linux/skbuff.h>
#include <asm/div64.h>
#include <linux/math64.h>

#define ELASTIC_SCALE 6
#define ELASTIC_UNIT (1 << ELASTIC_SCALE)

#define ALPHA_SHIFT	7
#define ALPHA_SCALE	(1u<<ALPHA_SHIFT)
#define ALPHA_MIN	((3*ALPHA_SCALE)/10)	/* ~0.3 */
#define ALPHA_MAX	(10*ALPHA_SCALE)	/* 10.0 */
#define ALPHA_BASE	ALPHA_SCALE		/* 1.0 */
#define RTT_MAX		(U32_MAX / ALPHA_MAX)	/* 3.3 secs */

#define BETA_SHIFT	6
#define BETA_SCALE	(1u<<BETA_SHIFT)
#define BETA_MIN	(BETA_SCALE/8)		/* 0.125 */
#define BETA_MAX	(BETA_SCALE/2)		/* 0.5 */
#define BETA_BASE	BETA_MAX

#define BASE_RTT_RESET_INTERVAL (10 * HZ) /* 10 seconds for base_rtt reset */

static const u32 bbr_cwnd_min_target = 4;

static int win_thresh __read_mostly = 20; /* Increased threshold for adaptive alpha/beta */
module_param(win_thresh, int, 0);
MODULE_PARM_DESC(win_thresh, "Window threshold for starting adaptive sizing");

static int theta __read_mostly = 8; /* Increased RTT count for full alpha growth recovery */
module_param(theta, int, 0);
MODULE_PARM_DESC(theta, "# of fast RTT's before full growth");

struct elastic {
	u64	sum_rtt;	/* sum of rtt's measured within last rtt */

	u32	rtt_max;        /* Max RTT used for wwf, decays */
	u32	rtt_curr;
	u32	base_rtt;	/* min of all rtt in usec */
	u32	max_rtt;	/* max rtt of the current round for alpha/beta */
	u32	alpha;		/* Additive increase */
	u32	beta;		/* Muliplicative decrease */
	u32 next_rtt_delivered; /* scb->tx.delivered at end of round */
	u32	prior_cwnd;	/* prior cwnd upon entering loss recovery */

	u16	cnt_rtt;	/* # of rtts measured within last rtt */
	u16	acked;		/* # packets acked by current ACK */

	u8	rtt_above;	/* average rtt has gone above threshold */
	u8	rtt_low;	/* # of rtts measurements below threshold */

	u32 last_base_rtt_reset_jiffies; /* For periodic base_rtt reset */
	u32 prev_ca_state:3;     /* CA state on previous ACK */
};

static void rtt_reset(struct sock *sk)
{
	struct elastic *ca = inet_csk_ca(sk);

	ca->cnt_rtt = 0;
	ca->sum_rtt = 0;

}

static void tcp_elastic_init(struct sock *sk)
{
	struct tcp_sock *tp = tcp_sk(sk);
	struct elastic *ca = inet_csk_ca(sk);

	ca->rtt_max = tp->srtt_us;
	ca->rtt_curr = ca->rtt_max+1;
	
	ca->alpha = ALPHA_MAX;
	ca->beta = BETA_BASE;
	ca->base_rtt = U32_MAX;
	ca->max_rtt = 0;

	ca->acked = 0;
	ca->rtt_low = 0;
	ca->rtt_above = 0;
	ca->last_base_rtt_reset_jiffies = jiffies;

	rtt_reset(sk);
	
	ca->prior_cwnd = TCP_INIT_CWND;
	tp->snd_ssthresh = TCP_INFINITE_SSTHRESH;
	ca->next_rtt_delivered = tp->delivered;
	ca->prev_ca_state = TCP_CA_Open;

	tp->snd_cwnd = 2;
	tp->snd_cwnd_clamp = 65535;

}

/* Calculate value scaled by beta */
static inline u32 calculate_beta_scaled_value(struct sock *sk, u32 value)
{
	const struct elastic *ca = inet_csk_ca(sk);

	return (value * ca->beta) >> BETA_SHIFT;
}

static u32 tcp_elastic_ssthresh(struct sock *sk)
{

	const struct tcp_sock *tp = tcp_sk(sk);
	struct elastic *ca = inet_csk_ca(sk);
	
	ca->prior_cwnd = tp->snd_cwnd;

	return max(tp->snd_cwnd - calculate_beta_scaled_value(sk, tp->snd_cwnd), 2U);
	
}

/* Maximum queuing delay */
static inline u32 max_delay(const struct elastic *ca)
{
	return ca->max_rtt - ca->base_rtt;
}

/* Average queuing delay */
static inline u32 avg_delay(const struct elastic *ca)
{
	u64 t = ca->sum_rtt;

	do_div(t, ca->cnt_rtt);
	return t - ca->base_rtt;
}

static u32 alpha(struct elastic *ca, u32 da, u32 dm)
{
	u32 d1 = dm / 100;	/* Low threshold */

	if (da <= d1) {
		/* If never got out of low delay zone, then use max */
		if (!ca->rtt_above)
			return ALPHA_MAX;
		/* Wait for 5 good RTT's before allowing alpha to go alpha max.
		 * This prevents one good RTT from causing sudden window increase.
		 */

		if (++ca->rtt_low < theta)
			return ca->alpha;

		ca->rtt_low = 0;
		ca->rtt_above = 0;
		return ALPHA_MAX;
	}

	ca->rtt_above = 1;

	dm -= d1;
	da -= d1;
	
	return (dm * ALPHA_MAX) /
		(dm + (da  * (ALPHA_MAX - ALPHA_MIN)) / ALPHA_MIN);
}

static u32 beta(u32 da, u32 dm)
{
	u32 d2, d3;

	d2 = dm / 10;
	if (da <= d2)
		return BETA_MIN;

	d3 = (8 * dm) / 10;
	if (da >= d3 || d3 <= d2)
		return BETA_MAX;

	/*
	 * Based on:
	 *
	 *       bmin d3 - bmax d2
	 * k3 = -------------------
	 *         d3 - d2
	 *
	 *       bmax - bmin
	 * k4 = -------------
	 *         d3 - d2
	 *
	 * b = k3 + k4 da
	 */
	return (BETA_MIN * d3 - BETA_MAX * d2 + (BETA_MAX - BETA_MIN) * da)
		/ (d3 - d2);
}

static void update_params(struct sock *sk)
{
	const struct tcp_sock *tp = tcp_sk(sk);
	struct elastic *ca = inet_csk_ca(sk);

	if (tp->snd_cwnd < win_thresh) {
		ca->alpha = ALPHA_BASE;
		ca->beta = BETA_BASE;
	} else if (ca->cnt_rtt > 0) {
		u32 dm = max_delay(ca);
		u32 da = avg_delay(ca);

		ca->alpha = alpha(ca, da, dm);
		ca->beta = beta(da, dm);
	}

	if (!before(jiffies, ca->last_base_rtt_reset_jiffies + BASE_RTT_RESET_INTERVAL)) {
		if (ca->max_rtt > tp->srtt_us)
			ca->rtt_max = ca->max_rtt;

		ca->max_rtt = ca->base_rtt;
		ca->base_rtt = U32_MAX;
		ca->last_base_rtt_reset_jiffies = jiffies;
	}

	rtt_reset(sk);
}

static void tcp_elastic_reset(struct sock *sk)
{
	struct elastic *ca = inet_csk_ca(sk);

	ca->alpha = ALPHA_BASE;
	ca->beta = BETA_BASE;
	ca->rtt_low = 0;
	ca->rtt_above = 0;
	rtt_reset(sk);
}

static void tcp_elastic_set_state(struct sock *sk, u8 new_state)
{

	struct elastic *ca = inet_csk_ca(sk);

	if (new_state == TCP_CA_Loss) {

		ca->prev_ca_state = TCP_CA_Loss;

		tcp_elastic_reset(sk);

	}
}

static void tcp_elastic_pkts_acked(struct sock *sk, const struct rate_sample *rs)
{
	struct elastic *ca = inet_csk_ca(sk);

	u32 rtt_us = rs->rtt_us;

	/* dup ack, no rtt sample */
	if (rtt_us < 0)
		return;

	/* ignore bogus values, this prevents wraparound in alpha math */
	if (rtt_us > RTT_MAX)
		rtt_us = RTT_MAX;


	ca->rtt_curr = rtt_us;

	if (ca->rtt_curr > ca->rtt_max) {
		ca->rtt_max = ca->rtt_curr;
	}

	ca->base_rtt = min_not_zero(ca->base_rtt, rtt_us);

	/* and max */
	if (ca->max_rtt < rtt_us)
		ca->max_rtt = rtt_us;

	++ca->cnt_rtt;
	ca->sum_rtt += rtt_us;
}

static void tcp_elastic_cong_avoid(struct sock *sk, u32 ack, u32 acked)
{
	struct tcp_sock *tp = tcp_sk(sk);
	struct elastic *ca = inet_csk_ca(sk);

	if (!tcp_is_cwnd_limited(sk))
		return;

	if (tcp_in_slow_start(tp)) {
		tcp_slow_start(tp, acked);
	} else {
		u64 wwf64;
		u32 wwf;
		u32 d;

		if (ca->rtt_curr > ca->base_rtt) {
			d = (ca->rtt_curr >> 1) + (ca->base_rtt >> 1);
		} else {
			d = ca->rtt_curr+1;
		}

		wwf64 = tp->snd_cwnd * ELASTIC_UNIT * ELASTIC_UNIT * ca->rtt_max / d;
		wwf64 = int_sqrt64(wwf64);
		wwf = wwf64 >> ELASTIC_SCALE;
		wwf = max(wwf, acked);

		tcp_cong_avoid_ai(tp, tp->snd_cwnd, wwf);
	}

}

static void tcp_elastic_update(struct sock *sk, const struct rate_sample *rs)
{
	const struct tcp_sock *tp = tcp_sk(sk);
	struct elastic *ca = inet_csk_ca(sk);

	ca->acked = rs->acked_sacked;

	if (rs->delivered < 0 || rs->interval_us <= 0)
		return; /* Not a valid observation */	

	/* See if we've reached the next RTT */
	if (!before(rs->prior_delivered, ca->next_rtt_delivered)) {
		ca->next_rtt_delivered = tp->delivered;
		update_params(sk);
	}

	/* Record the last non-app-limited or the highest app-limited bw */
	if (!rs->is_app_limited ||
	    ((u64)rs->delivered * tp->rate_interval_us >=
	     (u64)tp->rate_delivered * rs->interval_us)) {
		tcp_elastic_pkts_acked(sk, rs);
	}

}

static void tcp_elastic_cong_control(struct sock *sk, const struct rate_sample *rs)
{
	struct tcp_sock *tp = tcp_sk(sk);
	struct elastic *ca = inet_csk_ca(sk);

	u8 prev_state = ca->prev_ca_state, state = inet_csk(sk)->icsk_ca_state;
	u32 cwnd = tp->snd_cwnd;

	tcp_elastic_update(sk, rs);

	if (!rs->acked_sacked)
		goto done;  /* no packet fully ACKed; just apply caps */	

	if (state == TCP_CA_Open) /* Let cong_control adjustments run even if hybla_en is true */
		goto done;

	ca->prev_ca_state = state;
	
	/* An ACK for P pkts should release at most 2*P packets. We do this
	 * in two steps. First, here we deduct the number of lost packets.
	 * Then, in bbr_set_cwnd() we slow start up toward the target cwnd.
	 */
	if (rs->losses > 0)
		cwnd = max_t(u32, cwnd - rs->losses, 1);

	if (state == TCP_CA_Recovery && prev_state != TCP_CA_Recovery) {
		ca->next_rtt_delivered = tp->delivered;  /* start round now */
		/* Cut unused cwnd from app behavior, TSQ, or TSO deferral: */
		cwnd = max(cwnd, tcp_packets_in_flight(tp) + rs->acked_sacked);
		goto done;
	} else if (prev_state >= TCP_CA_Recovery && state < TCP_CA_Recovery) {
		/* Exiting loss recovery; restore cwnd saved before recovery. */
		cwnd = max(cwnd, ca->prior_cwnd);
	}

	cwnd = max(cwnd, tcp_packets_in_flight(tp) + bbr_cwnd_min_target);

done:
	tp->snd_cwnd = min(cwnd, tp->snd_cwnd_clamp);

}

static struct tcp_congestion_ops tcp_elastic __read_mostly = {
	.name		= "elastic",
	.owner		= THIS_MODULE,
	.init		= tcp_elastic_init,
	.ssthresh   = tcp_elastic_ssthresh,
	.undo_cwnd	= tcp_reno_undo_cwnd,
	.cong_avoid	= tcp_elastic_cong_avoid,
	.cong_control  = tcp_elastic_cong_control,
	.set_state	= tcp_elastic_set_state
};

static int __init elastic_register(void)
{
	BUILD_BUG_ON(sizeof(struct elastic) > ICSK_CA_PRIV_SIZE);
	return tcp_register_congestion_control(&tcp_elastic);
}

static void __exit elastic_unregister(void)
{
	tcp_unregister_congestion_control(&tcp_elastic);
}

module_init(elastic_register);
module_exit(elastic_unregister);

MODULE_LICENSE("GPL");
MODULE_DESCRIPTION("Elastic TCP");

